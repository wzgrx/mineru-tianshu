"""
MinerU Tianshu - LitServe Worker
å¤©æ¢ LitServe Worker (Fixed VLLM & Memory Optimization)
"""

import os
import json
import sys
import time
import threading
import signal
import atexit
from pathlib import Path
from typing import Optional
import multiprocessing
import importlib.util

# ============================================================================
# 1. ç¦ç”¨ LitServe å†…ç½® MCP (é¿å…å†²çª)
# ============================================================================
import litserve as ls
from litserve.connector import check_cuda_with_nvidia_smi
from utils import parse_list_arg

try:
    import litserve.mcp as ls_mcp
    from contextlib import asynccontextmanager

    class DummyMCPServer:
        def __init__(self, *args, **kwargs): pass
    
    class DummyMCPConnector:
        def __init__(self, *args, **kwargs): pass
        @asynccontextmanager
        async def lifespan(self, app): yield
        def connect_mcp_server(self, *args, **kwargs): pass

    ls_mcp.MCPServer = DummyMCPServer
    ls_mcp._LitMCPServerConnector = DummyMCPConnector
    if "litserve.mcp" in sys.modules:
        sys.modules["litserve.mcp"].MCPServer = DummyMCPServer
        sys.modules["litserve.mcp"]._LitMCPServerConnector = DummyMCPConnector
except Exception as e:
    import warnings
    warnings.warn(f"Failed to patch litserve.mcp: {e}")

from loguru import logger

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from task_db import TaskDB
from output_normalizer import normalize_output

# ============================================================================
# 2. å¼•æ“å¯ç”¨æ€§æ£€æµ‹
# ============================================================================

try:
    from markitdown import MarkItDown
    MARKITDOWN_AVAILABLE = True
except ImportError:
    MARKITDOWN_AVAILABLE = False
    logger.warning("âš ï¸  markitdown not available")

PADDLEOCR_AVAILABLE = importlib.util.find_spec("paddleocr") is not None
if PADDLEOCR_AVAILABLE:
    logger.info("âœ… PaddleOCR engine available")
else:
    logger.warning("âš ï¸  PaddleOCR not available (pip install paddleocr>=2.9.1)")

MINERU_PIPELINE_AVAILABLE = importlib.util.find_spec("mineru_pipeline") is not None
if MINERU_PIPELINE_AVAILABLE:
    logger.info("âœ… MinerU Pipeline available")

MINERU_VLM_AVAILABLE = importlib.util.find_spec("mineru.backend.vlm") is not None
if MINERU_VLM_AVAILABLE:
    logger.info("âœ… MinerU VLM available")

MINERU_HYBRID_AVAILABLE = importlib.util.find_spec("mineru.backend.hybrid") is not None
if MINERU_HYBRID_AVAILABLE:
    logger.info("âœ… MinerU Hybrid available")

SENSEVOICE_AVAILABLE = importlib.util.find_spec("audio_engines") is not None
VIDEO_ENGINE_AVAILABLE = importlib.util.find_spec("video_engines") is not None
WATERMARK_REMOVAL_AVAILABLE = importlib.util.find_spec("remove_watermark") is not None

# Format Engines
try:
    from format_engines import FormatEngineRegistry, FASTAEngine, GenBankEngine
    FormatEngineRegistry.register(FASTAEngine())
    FormatEngineRegistry.register(GenBankEngine())
    FORMAT_ENGINES_AVAILABLE = True
except ImportError:
    FORMAT_ENGINES_AVAILABLE = False


class MinerUWorkerAPI(ls.LitAPI):
    def __init__(
        self,
        paddleocr_vl_vllm_api_list=None,
        output_dir=None,
        poll_interval=0.5,
        enable_worker_loop=True,
        paddleocr_vl_vllm_engine_enabled=False,
    ):
        super().__init__()
        project_root = Path(__file__).parent.parent
        default_output = project_root / "data" / "output"
        self.output_dir = output_dir or os.getenv("OUTPUT_PATH", str(default_output))
        self.poll_interval = poll_interval
        self.enable_worker_loop = enable_worker_loop
        self.paddleocr_vl_vllm_engine_enabled = paddleocr_vl_vllm_engine_enabled
        self.paddleocr_vl_vllm_api_list = paddleocr_vl_vllm_api_list or []
        
        ctx = multiprocessing.get_context("spawn")
        self._global_worker_counter = ctx.Value("i", 0)

    def setup(self, device):
        with self._global_worker_counter.get_lock():
            my_global_index = self._global_worker_counter.value
            self._global_worker_counter.value += 1
        
        logger.info(f"ğŸ”¢ Worker #{my_global_index} setup on {device}")

        # è®¾ç½® CUDA_VISIBLE_DEVICES
        if "cuda:" in str(device):
            gpu_id = str(device).split(":")[-1]
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
            os.environ["MINERU_DEVICE_MODE"] = "cuda:0"

        # âœ… [å…³é”®ä¿®å¤] é™åˆ¶ vLLM æ˜¾å­˜å ç”¨ (è§£å†³ OOM æ ¸å¿ƒ)
        # 0.7 = 70% æ˜¾å­˜ç»™ vLLMï¼Œå‰©ä½™ 30% (çº¦7GB/24GB) ç»™ PaddleOCR/MinerU ä¸´æ—¶ä½¿ç”¨
        os.environ["VLLM_GPU_MEMORY_UTILIZATION"] = "0.7"
        # å¼ºåˆ¶ vLLM ä½¿ç”¨ä¸ PyTorch å…¼å®¹çš„æ˜¾å­˜åˆ†é…æ–¹å¼
        os.environ["VLLM_ALLOW_LONG_MAX_MODEL_LEN"] = "1"

        # é…ç½®æ¨¡å‹æº (ModelScope/HF)
        model_source = os.getenv("MODEL_DOWNLOAD_SOURCE", "auto").lower()
        if model_source == "modelscope":
             os.environ["MINERU_MODEL_SOURCE"] = "modelscope"
        
        self.device = device
        self.accelerator = "cuda" if "cuda" in str(device) else "cpu"
        self.engine_device = "cuda:0" if self.accelerator == "cuda" else "cpu"

        # å»¶è¿ŸåŠ è½½ MinerU VRAM Utils
        global get_vram, clean_memory
        from mineru.utils.model_utils import get_vram, clean_memory
        
        # åˆå§‹åŒ–æ•°æ®åº“
        db_path = os.getenv("DATABASE_PATH", "/app/data/db/mineru_tianshu.db")
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        self.task_db = TaskDB(db_path)
        
        # å¼•æ“å®ä¾‹ç¼“å­˜
        self.markitdown = MarkItDown() if MARKITDOWN_AVAILABLE else None
        self.mineru_pipeline_engine = None
        self.paddleocr_vl_engine = None
        self.paddleocr_vllm_engine = None  # âœ… vLLM å¼•æ“å¥æŸ„
        self.sensevoice_engine = None
        self.video_engine = None
        self.watermark_handler = None

        # åˆå§‹åŒ–æ°´å°å¼•æ“
        if WATERMARK_REMOVAL_AVAILABLE and self.accelerator == "cuda":
            try:
                from remove_watermark.pdf_watermark_handler import PDFWatermarkHandler
                self.watermark_handler = PDFWatermarkHandler(device="cuda:0", use_lama=True)
                logger.info("âœ… Watermark engine ready")
            except Exception as e:
                logger.error(f"âŒ Watermark engine failed: {e}")

        # å¯åŠ¨å¾ªç¯
        self.running = True
        self.current_task_id = None
        if self.enable_worker_loop:
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()

    def _worker_loop(self):
        """Worker ä¸»å¾ªç¯"""
        logger.info(f"ğŸ” Worker loop started (interval={self.poll_interval}s)")
        while self.running:
            try:
                task = self.task_db.get_next_task(worker_id=self.worker_id)
                if task:
                    self.current_task_id = task["task_id"]
                    logger.info(f"ğŸ“¥ Processing task: {task['task_id']} ({task['backend']})")
                    try:
                        self._process_task(task)
                        logger.info(f"âœ… Task completed: {task['task_id']}")
                    except Exception as e:
                        logger.error(f"âŒ Task failed: {e}")
                        logger.exception(e)
                    finally:
                        self.current_task_id = None
                else:
                    time.sleep(self.poll_interval)
            except Exception as e:
                logger.error(f"Loop error: {e}")
                time.sleep(1)

    def _process_task(self, task: dict):
        """æ ¸å¿ƒä»»åŠ¡åˆ†å‘é€»è¾‘"""
        task_id = task["task_id"]
        file_path = task["file_path"]
        options = json.loads(task.get("options", "{}"))
        backend = task.get("backend", "auto")

        try:
            # 1. é¢„å¤„ç†ï¼šPDF è½¬æ¢ / æ°´å°å»é™¤ / æ‹†åˆ†
            file_ext = Path(file_path).suffix.lower()
            
            # Office è½¬ PDF
            if file_ext in [".docx", ".xlsx", ".pptx"] and options.get("convert_office_to_pdf"):
                file_path = self._convert_office_to_pdf(file_path)
                file_ext = ".pdf"
            
            # PDF æ‹†åˆ† (ä»…é’ˆå¯¹ PDF ä¸”éå­ä»»åŠ¡)
            if file_ext == ".pdf" and not task.get("parent_task_id"):
                 if self._should_split_pdf(task_id, file_path, task, options):
                     return # å·²æ‹†åˆ†ä¸ºå­ä»»åŠ¡
            
            # å»é™¤æ°´å°
            if file_ext == ".pdf" and options.get("remove_watermark") and self.watermark_handler:
                file_path = str(self._preprocess_remove_watermark(file_path, options))

            # 2. å¼•æ“è·¯ç”±
            result = None
            
            # === MinerU ç³»åˆ— ===
            if backend == "pipeline":
                if not MINERU_PIPELINE_AVAILABLE: raise ValueError("MinerU Pipeline missing")
                result = self._process_with_mineru(file_path, options)
            
            elif backend == "vlm-auto-engine":
                if not MINERU_VLM_AVAILABLE: raise ValueError("MinerU VLM missing")
                result = self._process_with_mineru_vlm(file_path, options)

            elif backend == "hybrid-auto-engine":
                if not MINERU_HYBRID_AVAILABLE: raise ValueError("MinerU Hybrid missing")
                result = self._process_with_mineru_hybrid(file_path, options)

            # === [æ–°å¢] PaddleOCR VLLM åŠ é€Ÿç‰ˆ ===
            elif backend == "paddleocr-vl-vllm":
                if not importlib.util.find_spec("vllm"): raise ValueError("vLLM module not found")
                logger.info(f"ğŸš€ Processing with PaddleOCR-VL (vLLM): {file_path}")
                result = self._process_with_paddleocr_vllm(file_path, options)

            # === PaddleOCR æœ¬åœ°ç‰ˆ ===
            elif backend in ["paddleocr-vl", "paddleocr-vl-0.9b", "paddleocr-vl-1.5-0.9b", 
                             "pp-ocrv5", "pp-structurev3", "pp-chatocrv4"]:
                if not PADDLEOCR_AVAILABLE: raise ValueError("PaddleOCR missing")
                options['model_type'] = backend 
                result = self._process_with_paddleocr(file_path, options)

            # === éŸ³è§†é¢‘/å…¶ä»– ===
            elif backend == "sensevoice":
                if not SENSEVOICE_AVAILABLE: raise ValueError("SenseVoice missing")
                result = self._process_audio(file_path, options)
            
            elif backend == "video":
                if not VIDEO_ENGINE_AVAILABLE: raise ValueError("Video engine missing")
                result = self._process_video(file_path, options)
            
            # === è‡ªåŠ¨é€‰æ‹© ===
            elif backend == "auto":
                if file_ext == ".pdf": # é»˜è®¤ç”¨ Pipeline
                    result = self._process_with_mineru(file_path, options)
                elif file_ext in [".jpg", ".png"]: # é»˜è®¤ç”¨ PaddleOCR-VL
                    options['model_type'] = 'paddleocr-vl'
                    result = self._process_with_paddleocr(file_path, options)
                elif self.markitdown: # Office/Text
                    result = self._process_with_markitdown(file_path)
                else:
                    raise ValueError("No suitable engine found for auto mode")

            else:
                raise ValueError(f"Unknown backend: {backend}")

            # 3. ç»“æœå¤„ç†
            if result:
                self.task_db.update_task_status(
                    task_id=task_id,
                    status="completed",
                    result_path=result["result_path"]
                )
                # å¤„ç†å­ä»»åŠ¡åˆå¹¶é€»è¾‘...
                if task.get("parent_task_id"):
                    parent_id = self.task_db.on_child_task_completed(task_id)
                    if parent_id: self._merge_parent_task_results(parent_id)

            if "cuda" in str(self.device): clean_memory()

        except Exception as e:
            self.task_db.update_task_status(task_id, "failed", error_message=str(e))
            if task.get("parent_task_id"):
                self.task_db.on_child_task_failed(task_id, str(e))
            raise

    # ==========================================================
    # å…·ä½“å¤„ç†æ–¹æ³•
    # ==========================================================
    
    def _process_with_paddleocr_vllm(self, file_path: str, options: dict) -> dict:
        """è°ƒç”¨ PaddleOCR vLLM åŠ é€Ÿå¼•æ“"""
        if self.paddleocr_vllm_engine is None:
            # å»¶è¿Ÿå¯¼å…¥ï¼Œé˜²æ­¢å¯åŠ¨æ—¶å ç”¨æ˜¾å­˜
            from paddleocr_vl_vllm.engine import PaddleOCRVLLMEngine
            self.paddleocr_vllm_engine = PaddleOCRVLLMEngine()
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è°ƒç”¨ VLLM å¼•æ“çš„è§£ææ–¹æ³•
        result = self.paddleocr_vllm_engine.parse(file_path, str(output_dir), **options)
        
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_with_paddleocr(self, file_path: str, options: dict) -> dict:
        """ç»Ÿä¸€è°ƒç”¨ PaddleOCR å¼•æ“"""
        if self.paddleocr_vl_engine is None:
            from paddleocr_vl.engine import get_engine
            self.paddleocr_vl_engine = get_engine() # å•ä¾‹è·å–
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        
        # ä¼ é€’ options (åŒ…å« model_type)
        result = self.paddleocr_vl_engine.parse(file_path, str(output_dir), **options)
        
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_with_mineru(self, file_path: str, options: dict) -> dict:
        if self.mineru_pipeline_engine is None:
            from mineru_pipeline import MinerUPipelineEngine
            self.mineru_pipeline_engine = MinerUPipelineEngine(device=self.engine_device)
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        result = self.mineru_pipeline_engine.parse(file_path, output_path=str(output_dir), options=options)
        normalize_output(Path(result["result_path"]))
        return {"result_path": result["result_path"], "content": result["markdown"]}

    def _process_with_mineru_vlm(self, file_path: str, options: dict) -> dict:
        from mineru.backend.vlm.vlm_analyze import doc_analyze
        from mineru.data.data_reader_writer import FileBasedDataWriter
        from mineru.backend.vlm.vlm_middle_json_mkcontent import mid_json_to_markdown
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        
        with open(file_path, "rb") as f: content = f.read()
        writer = FileBasedDataWriter(str(output_dir))
        
        middle_json, _ = doc_analyze(content, writer, backend="transformers")
        md = mid_json_to_markdown(middle_json)
        
        (output_dir / "result.md").write_text(md, encoding="utf-8")
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": md}

    def _process_with_mineru_hybrid(self, file_path: str, options: dict) -> dict:
        from mineru.backend.hybrid.hybrid_analyze import doc_analyze
        from mineru.data.data_reader_writer import FileBasedDataWriter
        from mineru.backend.pipeline.pipeline_middle_json_mkcontent import mid_json_to_markdown

        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        with open(file_path, "rb") as f: content = f.read()
        writer = FileBasedDataWriter(str(output_dir))

        middle_json, _, _ = doc_analyze(
            content, writer, 
            language=options.get("lang", "ch"),
            parse_method=options.get("method", "auto")
        )
        md = mid_json_to_markdown(middle_json)
        (output_dir / "result.md").write_text(md, encoding="utf-8")
        
        normalize_output(output_dir)
        return {"result_path": str(output_dir), "content": md}

    def _process_audio(self, file_path: str, options: dict) -> dict:
        # ç®€å•å®ç°ï¼Œå‡è®¾å·²åœ¨ audio_engines ä¸­å®ç°
        from audio_engines.sensevoice_engine import SenseVoiceEngine
        if self.sensevoice_engine is None:
            self.sensevoice_engine = SenseVoiceEngine(device=self.engine_device)
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        
        result = self.sensevoice_engine.transcribe(file_path, options)
        (output_dir / "result.json").write_text(json.dumps(result, ensure_ascii=False), encoding="utf-8")
        
        return {"result_path": str(output_dir), "content": str(result)}

    def _process_video(self, file_path: str, options: dict) -> dict:
        # ç®€å•å®ç°ï¼Œå‡è®¾å·²åœ¨ video_engines ä¸­å®ç°
        from video_engines.video_engine import VideoEngine
        if self.video_engine is None:
            self.video_engine = VideoEngine()
            
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        
        result = self.video_engine.process(file_path, str(output_dir), options)
        return {"result_path": str(output_dir), "content": "Video processed"}

    def _convert_office_to_pdf(self, file_path):
        # ç®€å•å ä½ç¬¦ï¼Œå®é™…éœ€è°ƒç”¨ LibreOffice æˆ– Pandoc
        # å‡è®¾ docker é•œåƒä¸­å·²æœ‰ç›¸å…³å·¥å…·
        return file_path
        
    def _should_split_pdf(self, task_id, file_path, task, options):
        # ç®€å•å ä½ç¬¦
        return False
    
    def _preprocess_remove_watermark(self, file_path, options):
         # ç®€å•å ä½ç¬¦
        return file_path

    def _merge_parent_task_results(self, parent_id):
        pass

    def _process_with_markitdown(self, file_path):
        return {"result_path": "", "content": "MarkItDown not implemented"}

    def decode_request(self, request):
        return request.get("action", "health")

    def predict(self, action):
        if action == "health":
            return {"status": "healthy", "worker_id": self.worker_id}
        return {"status": "ok"}

def start_litserve_workers(**kwargs):
    api = MinerUWorkerAPI(**kwargs)
    server = ls.LitServer(api, accelerator="auto", workers_per_device=1)
    server.run(port=kwargs.get('port', 8001))

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    # ç®€åŒ–ç‰ˆå…¥å£
    start_litserve_workers(output_dir=None)
